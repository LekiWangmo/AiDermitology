{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPltB0MXDX3o3XRo5phuwKh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LekiWangmo/AiDermitology/blob/main/Untitled29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUaUmiPfy1HN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!pip install -q torch_snippets pytorch_model_summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_snippets import *\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "# define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "-fAetSe4y3TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfms = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224,\n",
        "0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "wqjLUxVIy-oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "# Define the transformation for image and mask\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert image to tensor\n",
        "    transforms.Resize((224, 224))  # Resize images to a fixed size (224x224)\n",
        "])\n",
        "\n",
        "class VOCSegData(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load VOC dataset from torchvision\n",
        "        self.voc_dataset = datasets.VOCSegmentation(root=root_dir, year='2012', image_set=split, download=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.voc_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = self.voc_dataset[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)  # Apply same transformation to mask\n",
        "\n",
        "        mask = mask.squeeze(0).long()  # Remove channel dimension and convert to LongTensor\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Custom collate_fn to handle batch formation\n",
        "    images, masks = zip(*batch)\n",
        "\n",
        "    # Stack all the images and masks\n",
        "    images = torch.stack(images, dim=0)\n",
        "    masks = torch.stack(masks, dim=0)\n",
        "\n",
        "    return images, masks\n",
        "\n"
      ],
      "metadata": {
        "id": "4D1PD9jxze7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Initialize the dataset\n",
        "root_dir = './data'  # or wherever you're storing it\n",
        "trn_ds = VOCSegData(root_dir=root_dir, split='train', transform=transform)\n",
        "\n",
        "# Get the 4th sample\n",
        "image, mask = trn_ds[3]  # Index 3 means 4th sample\n",
        "\n",
        "# Convert tensor to PIL image for visualization (if needed)\n",
        "image_np = TF.to_pil_image(image)\n",
        "mask_np = TF.to_pil_image(mask.to(torch.uint8))\n",
        "\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image_np)\n",
        "plt.axis('off')\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.title(\"Segmentation Mask\")\n",
        "# plt.imshow(mask_np, cmap='gray')  # Use cmap for grayscale masks\n",
        "# plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BAexa8hc-79s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set up the train and validation data loaders\n",
        "trn_ds = VOCSegData(root_dir='./data', split='train', transform=transform)\n",
        "val_ds = VOCSegData(root_dir='./data', split='val', transform=transform)\n",
        "\n",
        "trn_dl = DataLoader(trn_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_dl = DataLoader(val_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "O9H4RN3H0UFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297676ec-b385-4fef-d09d-5744a61230a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.00G/2.00G [01:55<00:00, 17.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Convolution Block\n",
        "def conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "# Upsampling Convolution Block\n",
        "def up_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "sUorckIU1Ezx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg16_bn\n",
        "\n",
        "# U-Net Architecture with VGG16-BN Encoder\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, pretrained=True, out_channels=12):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = vgg16_bn(pretrained=pretrained).features\n",
        "\n",
        "        # Encoder blocks\n",
        "        self.block1 = nn.Sequential(*self.encoder[:6])     # Conv1\n",
        "        self.block2 = nn.Sequential(*self.encoder[6:13])   # Conv2\n",
        "        self.block3 = nn.Sequential(*self.encoder[13:20])  # Conv3\n",
        "        self.block4 = nn.Sequential(*self.encoder[20:27])  # Conv4\n",
        "        self.block5 = nn.Sequential(*self.encoder[27:34])  # Conv5\n",
        "\n",
        "        self.bottleneck = nn.Sequential(*self.encoder[34:])  # Remaining layers of VGG\n",
        "        self.conv_bottleneck = conv(512, 1024)  # Custom bottleneck conv\n",
        "\n",
        "        # Decoder blocks\n",
        "        self.up_conv6 = up_conv(1024, 512)\n",
        "        self.conv6 = conv(512 + 512, 512)\n",
        "\n",
        "        self.up_conv7 = up_conv(512, 256)\n",
        "        self.conv7 = conv(256 + 512, 256)\n",
        "\n",
        "        self.up_conv8 = up_conv(256, 128)\n",
        "        self.conv8 = conv(128 + 256, 128)\n",
        "\n",
        "        self.up_conv9 = up_conv(128, 64)\n",
        "        self.conv9 = conv(64 + 128, 64)\n",
        "\n",
        "        self.up_conv10 = up_conv(64, 32)\n",
        "        self.conv10 = conv(32 + 64, 32)\n",
        "\n",
        "        self.conv11 = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(block5)\n",
        "        x = self.conv_bottleneck(bottleneck)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.up_conv6(x)\n",
        "        x = torch.cat([x, block5], dim=1)\n",
        "        x = self.conv6(x)\n",
        "\n",
        "        x = self.up_conv7(x)\n",
        "        x = torch.cat([x, block4], dim=1)\n",
        "        x = self.conv7(x)\n",
        "\n",
        "        x = self.up_conv8(x)\n",
        "        x = torch.cat([x, block3], dim=1)\n",
        "        x = self.conv8(x)\n",
        "\n",
        "        x = self.up_conv9(x)\n",
        "        x = torch.cat([x, block2], dim=1)\n",
        "        x = self.conv9(x)\n",
        "\n",
        "        x = self.up_conv10(x)\n",
        "        x = torch.cat([x, block1], dim=1)\n",
        "        x = self.conv10(x)\n",
        "\n",
        "        x = self.conv11(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VL0VyY__6F-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Cross-Entropy Loss for multi-class segmentation\n",
        "# ce = nn.CrossEntropyLoss()\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# CrossEntropyLoss (already handles log-softmax + NLL loss internally)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "def UnetLoss(preds, targets):\n",
        "    # preds: shape (N, C, H, W), targets: shape (N, H, W)\n",
        "    ce_loss = ce(preds, targets.long())  # Ensure targets are LongTensor for CE\n",
        "    acc = (torch.argmax(preds, dim=1) == targets).float().mean()\n",
        "    return ce_loss, acc\n",
        "\n"
      ],
      "metadata": {
        "id": "BCndqhVo9Egh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_batch(model, data, optimizer, criterion):\n",
        "#     model.train()  # Set the model to training mode\n",
        "#     ims, ce_masks = data  # Unpack the batch into input images and ground truth masks\n",
        "#     _masks = model(ims)   # Forward pass to get predictions\n",
        "#     optimizer.zero_grad()  # Clear previous gradients\n",
        "#     loss, acc = criterion(_masks, ce_masks)  # Compute loss and accuracy\n",
        "#     loss.backward()  # Backpropagate the loss\n",
        "#     optimizer.step()  # Update model parameters\n",
        "#     return loss.item(), acc.item()  # Return scalar values for tracking\n",
        "\n",
        "# def train_batch(model, data, optimizer, criterion):\n",
        "#     model.train()\n",
        "#     ims, ce_masks = data\n",
        "#     ims, ce_masks = ims.to(device), ce_masks.to(device)  # Move to same device as model\n",
        "\n",
        "#     _masks = model(ims)\n",
        "#     optimizer.zero_grad()\n",
        "#     loss, acc = criterion(_masks, ce_masks)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "def train_batch(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    ims, ce_masks = data #Unpacks the input data tuple into\n",
        "    _masks = model(ims) #Passes the input images through the\n",
        "    optimizer.zero_grad() # Clears the gradients of all\n",
        "    loss, acc = criterion(_masks, ce_masks) #Calculates the\n",
        "    loss.backward() #Backpropagates the gradients.\n",
        "    optimizer.step()\n",
        "    return loss.item(), acc.item()\n",
        "\n",
        "    return loss.item(), acc.item()\n",
        "\n"
      ],
      "metadata": {
        "id": "gnNZ9pWS9z6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @torch.no_grad()\n",
        "# def validate_batch(model, data, criterion):\n",
        "#     model.eval()\n",
        "#     ims, masks = data\n",
        "#     ims, masks = ims.to(device), masks.to(device)\n",
        "\n",
        "#     outputs = model(ims)\n",
        "#     loss = criterion(outputs, masks)\n",
        "\n",
        "#     preds = torch.argmax(outputs, dim=1)\n",
        "#     acc = (preds == masks).float().mean()  # Works if dims match\n",
        "\n",
        "#     return loss.item(), acc.item()\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def validate_batch(model, data, criterion):\n",
        "#     model.eval()\n",
        "#     ims, masks = data\n",
        "#     ims, masks = ims.to(device), masks.to(device)\n",
        "\n",
        "#     outputs = model(ims)\n",
        "#     loss, acc = criterion(outputs, masks)  # Unpack both here\n",
        "#     return loss.item(), acc.item()\n",
        "\n",
        "@torch.no_grad()#Decorator that disables gradient computation\n",
        "def validate_batch(model, data, criterion):\n",
        "    model.eval()\n",
        "    ims, masks = data\n",
        "    _masks = model(ims)\n",
        "    loss, acc = criterion(_masks, masks)\n",
        "    return loss.item(), acc.item()\n",
        "\n"
      ],
      "metadata": {
        "id": "8KqUG0Uz-IsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet().to(device)\n",
        "criterion = UnetLoss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "n_epochs = 20\n"
      ],
      "metadata": {
        "id": "rq7J41r7-KjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1844923a-4189-4eae-bac7-a1bf3cda90a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 154MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Report:\n",
        "#     def __init__(self, n_epochs):\n",
        "#         self.n_epochs = n_epochs\n",
        "#         self.epoch_data = []\n",
        "\n",
        "#     def record(self, step, **metrics):\n",
        "#         self.epoch_data.append((step, metrics))  # No printing per batch\n",
        "\n",
        "#     def report_avgs(self, epoch):\n",
        "#         trn_loss, trn_acc, val_loss, val_acc = [], [], [], []\n",
        "#         for _, data in self.epoch_data:\n",
        "#             if 'trn_loss' in data: trn_loss.append(data['trn_loss'])\n",
        "#             if 'trn_acc' in data: trn_acc.append(data['trn_acc'])\n",
        "#             if 'val_loss' in data: val_loss.append(data['val_loss'])\n",
        "#             if 'val_acc' in data: val_acc.append(data['val_acc'])\n",
        "\n",
        "#         print(f\"Epoch {epoch}/{self.n_epochs}:\")\n",
        "#         if trn_loss:\n",
        "#             print(f\"  Train Loss: {sum(trn_loss)/len(trn_loss):.4f}, Accuracy: {sum(trn_acc)/len(trn_acc):.4f}\")\n",
        "#         if val_loss:\n",
        "#             print(f\"  Val   Loss: {sum(val_loss)/len(val_loss):.4f}, Accuracy: {sum(val_acc)/len(val_acc):.4f}\")\n",
        "\n",
        "#         self.epoch_data = []  # Reset for next epoch\n"
      ],
      "metadata": {
        "id": "jq7Jf5DW-lwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the purpose of Report class is to keep track/log and display\n",
        "# training and validation metrics during training.\n",
        "log = Report(n_epochs)\n",
        "for ex in range(n_epochs): #Iterates over the specified number of epochs.\n",
        "    N = len(trn_dl) #Sets N to the total number of batches in the training data loader (trn_dl).\n",
        "    for bx, data in enumerate(trn_dl): #Iterates over batches in the training data loader.\n",
        "      loss, acc = train_batch(model, data, optimizer, criterion)\n",
        "      #Performs a training step on the current batch and obtains the training loss and accuracy.\n",
        "      log.record(ex+(bx+1)/N, trn_loss=loss, trn_acc=acc, end='\\r') #Records the training loss and accuracy for the current batch The code then repeats a similar process for the validation set.\n",
        "    N = len(val_dl)\n",
        "    for bx, data in enumerate(val_dl):\n",
        "      loss, acc = validate_batch(model, data, criterion)\n",
        "      log.record(ex+(bx+1)/N, val_loss=loss, val_acc=acc, end='\\r')\n",
        "      #Calls the report_avgs method of the Report instance to report and display the average metrics for the epoch.\n",
        "    log.report_avgs(ex+1) # specifies the current epoch."
      ],
      "metadata": {
        "id": "JkPvuG-J_jqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mT2w6v8vGIFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log.plot_epochs(['trn_loss','val_loss'])\n",
        "log.plot_epochs(['trn_acc', 'val_acc'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "bmL67nWn_9Mh",
        "outputId": "184084fe-ae04-4ae8-fb94-e0e5d99a931b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Report' object has no attribute 'plot_epochs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-a72d792f89da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trn_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trn_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Report' object has no attribute 'plot_epochs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmKgqDBxBwHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}